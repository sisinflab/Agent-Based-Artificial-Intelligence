{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AGENT-BASED ARTIFICIAL INTELLIGENCE, POLIBA, A.Y. 2023/2024**\n",
        "\n",
        "**LOCAL SEARCH**\n",
        "\n",
        "List of Contributors:\n",
        "- Tommaso Di Noia\n",
        "- Alberto Carlo Maria Mancino\n",
        "- Vincenzo Paparella"
      ],
      "metadata": {
        "id": "wZnvfbmJAkK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction to Local Search"
      ],
      "metadata": {
        "id": "tue7RxouDm6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have studied how to solve search problems with uninformed and informed strategies. In both cases, our goal was to find a path, i.e., a sequence of actions, through the search space to reach a goal state.\n",
        "\n",
        "However, sometimes we care only about the final state, not the path to get there. For example, in the 8-queens problem, we care only about finding a valid final configuration of 8 queens such that the queens are not in conflict among them. Indeed, it is trivial to reconstruct the steps that created the valid configuration.\n",
        "\n",
        "In this context, **local search** algorithms operate by searching from an initial state to neighboring states solely, without keeping track of the paths, nor the set of states that have been reached.\n",
        "\n",
        "Practically, within local search algorithms, we design a **value function** such that the closer we are to the goal state, the higher is the value function. In this way, the maximum value of this function is associated to the goal state."
      ],
      "metadata": {
        "id": "AwciGHzBDple"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8-queens Problem"
      ],
      "metadata": {
        "id": "NDb1VqyFJ3Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate the local search algorithms, we will utilize the 8-queens problem as a toy-example.\n",
        "\n",
        "In this problem, we must place 8 queens on a 8x8 chess board so that no queen attacks another. A queen attacks any piece in the same row, column, or diagonal.\n",
        "\n",
        "Consequently, the goal state is a state in which the queens are placed such that the number of conflicts is 0.\n",
        "\n",
        "The initial state can be self-declared or chosen at random.\n",
        "\n",
        "Ok, let's start!"
      ],
      "metadata": {
        "id": "VwxBMV2cJ5YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Node's implementation"
      ],
      "metadata": {
        "id": "FqBOw7DiMa2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In local search algorithms, given a state, we move into one of its neighbors. Then, we move along a tree, as we did in uninformed and informed search strategies. However, we are not interested in the path along the tree.\n",
        "\n",
        "Given this observation, we will reuse the node's implementation seen for the tree search strategy. If you don't have familiarity with the following class, you should study tree and graph search first."
      ],
      "metadata": {
        "id": "4MG9IHsgMig3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, state, parent, action, cost, depth):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.cost = cost\n",
        "        self.depth = depth\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        A representation of the class. Useful with functions like print.\n",
        "        :return: a string\n",
        "        \"\"\"\n",
        "        return f'({self.state})'\n",
        "\n",
        "    def expand(self, state, action, cost=1):\n",
        "        \"\"\"\n",
        "        Given a new state returns a child tree node containing that state\n",
        "        :param new_state: state that will be contained by the node\n",
        "        :param action: action that led to the state\n",
        "        :param cost: cost of the path of the previous node\n",
        "        :return: a child node\n",
        "        \"\"\"\n",
        "        return Node(state=state,\n",
        "                    parent=self,\n",
        "                    action=action,\n",
        "                    cost=self.cost+cost,\n",
        "                    depth=self.depth+1)\n",
        "\n",
        "    def path(self):\n",
        "        \"\"\"\n",
        "         Returns the path from the root node to the actual node\n",
        "        :return: a list of actions\n",
        "        \"\"\"\n",
        "        path = []\n",
        "        node = self\n",
        "        while node.parent:\n",
        "            path.append(node.action)\n",
        "            node = node.parent\n",
        "        path = list(reversed(path))\n",
        "        return path\n"
      ],
      "metadata": {
        "id": "JwpFL7g5OWS6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modeling the environment"
      ],
      "metadata": {
        "id": "rcHL79vNOlSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we should model the chess board in such a way our machine can understand it.\n",
        "\n",
        "We can suppose to have one queen per column on our board. In this way, we already solve the conflicts among the queens due to the same column. In addition, this choice makes modeling the environment much easier.\n",
        "\n",
        "Indeed, now we can model our chess board as a tuple of eight elements in which each element corresponds to a queen. Specifically, the value of the element indicates the row of the queen, while the element's index within the tuple indicates the column of the queen.\n",
        "\n",
        "For instance, in the tuple (0,1,3,5,6,1,1) --- Please, observe that the maximum value of a row is 7 since the indexing starts from 0! --- :\n",
        "- the queen in column 0 is in row 0;\n",
        "- the queen in column 1 is in row 1;\n",
        "- the queen in column 2 is in row 3;\n",
        "- the queen in column 3 is in row 5;\n",
        "- and so son.\n",
        "\n",
        "However, as a good practice, we should not mix the concepts of environment and state. Actually, a populated tuple like the one in the previous example represents a state, i.e., a particular disposition of queens on the chess board.\n",
        "\n",
        "If we imagine the empty chess board, i.e., the environment only, it is an empty tuple (or a tuple with eight None within it (None, None, None, None, None, None, None, None)). Obviously, we are not required to implement an empty tuple! Then, we have just conceptualized our environment."
      ],
      "metadata": {
        "id": "aqFhpECYOa_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem formulation"
      ],
      "metadata": {
        "id": "34bFj8pLYmdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen in the tree and graph search, formulating the problem practically means modeling the agent. Therefore, we implement the EightQueensProblem Class in a similar way we have implemented the problem-related class for the tree/graph search:\n",
        "* the _successors_ function remains the same. It returns the reachable states from a state with the respective actions;\n",
        "* the _actions_ function, given a state, returns the list of possible actions. Here, the possible actions include moving each queen on a row of its associated column except for the row in which it stands on on the current state. In a list, we collect all the coordinates (col, row) of the chess board corresponding to the possible actions;\n",
        "* the _return_ function, given a state and an action, returns the reached state by performing the action. Given that an action is represented by the couple (col, row) as said above, this function is responsible of updating the row \"row\" of the column \"col\" of the current state;\n",
        "* the _cost_ function returns the cost of an action. In this problem, we do not have a policy for the cost. Then, it is always unitary;\n",
        "* the _goal_test_ function checks if the goal condition has been reached. For the 8-queens problem, the goal state is a state in which no queens are in conflict among them.\n",
        "\n",
        "Consequently, we implement the _conflicts_ function that, given a state, returns the number of conflicts among the queens. Then, given a queen, we have a conflict with a subsequent queen in three cases:\n",
        "- queens on the same column. We have already solved this conflict by supposing to have a chess board with only one queen on each column;\n",
        "- queens on the same row;\n",
        "- queens on the same diagonal. We notice that the conflict along the diagonals can be described as follows. If the diagonal has a positive slope, the sum between the rows' and columns' indices of the diagonal is constant. Then, two queens are in conflict if the sum of their indices is the same. If the diagonal has a negative slope, the difference between the rows' and columns' indices of the diagonal is constant. Then, two queens are in conflict if the difference of their indices is the same.\n",
        "\n",
        "Finally, we implement the _value_ function. We recall that the closer we are to the goal state, the higher is the value function. In this way, the maximum value of this function is associated to the goal state. For the 8-queens Problem, a valid value function would be the number of maximum conflicts minus the number of conflicts in the current state. In this way, if we are in the goal state, the function would return 0. The less is the number of conflicts (the closer we are to the goal state), the higher is the number returned."
      ],
      "metadata": {
        "id": "OiM0vdyaYpdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EightQueensProblem:\n",
        "\n",
        "    def __init__(self, initial_state=None):\n",
        "        if initial_state is None:\n",
        "            initial_state = self.random()\n",
        "        self.initial_state = initial_state\n",
        "        self.max_conflicts = sum([i for i in range(1, 8)])\n",
        "\n",
        "    def successors(self, state):\n",
        "        \"\"\"\n",
        "        Given a state returns the reachable states with the respective actions\n",
        "        :param state: actual state\n",
        "        :return: list of successor states and actions\n",
        "        \"\"\"\n",
        "        possible_actions = self.actions(state)\n",
        "        return [(self.result(state, a), a) for a in possible_actions]\n",
        "\n",
        "    def actions(self, state):\n",
        "        \"\"\"\n",
        "        Given a state returns the list of possible actions\n",
        "        :param state: actual state\n",
        "        :return: a list of actions\n",
        "        \"\"\"\n",
        "        actions = []\n",
        "        for col, queen in enumerate(state):\n",
        "            squares = list(range(0, 8))\n",
        "            squares.remove(queen)\n",
        "            # new_actions = list(zip(squares, [col]*len(squares)))\n",
        "            new_actions = list(zip([col] * len(squares), squares))\n",
        "            actions.extend(new_actions)\n",
        "        return actions\n",
        "\n",
        "    def result(self, state=None, action=None):\n",
        "        \"\"\"\n",
        "        Given a state and an action returns the reached state\n",
        "        :param state: actual state\n",
        "        :param action: chosen action\n",
        "        :return: reached state\n",
        "        \"\"\"\n",
        "        new_state = list(state)\n",
        "        col, new_row = action\n",
        "        new_state[col] = new_row\n",
        "        return tuple(new_state)\n",
        "\n",
        "    def conflicts(self, state):\n",
        "        \"\"\"\n",
        "        Given a state return the number of conflicts\n",
        "        :param state: a state\n",
        "        :return: number of conflicting queens\n",
        "        \"\"\"\n",
        "        conflicts = 0\n",
        "        for col in range(8):\n",
        "            queen = state[col]\n",
        "            for col1 in range(col+1, 8):\n",
        "                queen1 = state[col1]\n",
        "                if queen == queen1:\n",
        "                    conflicts += 1\n",
        "                if queen - col == queen1 - col1 or queen + col == queen1 + col1:\n",
        "                    conflicts += 1\n",
        "        return conflicts\n",
        "\n",
        "    def goal_test(self, state):\n",
        "        \"\"\"\n",
        "        Checks if the goal condition has been reached\n",
        "        :param state: actual state\n",
        "        :return: True if the goal condition is matched, False otherwise\n",
        "        \"\"\"\n",
        "        return self.conflicts(state) == 0\n",
        "\n",
        "    def cost(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns the cost of an action. In this problem the cost is always unitary.\n",
        "        :param state: a state\n",
        "        :param action: an action\n",
        "        :return: a cost\n",
        "        \"\"\"\n",
        "        return 1\n",
        "\n",
        "    def value(self, state):\n",
        "        \"\"\"\n",
        "        Returns the value of a state. This function is used for evaluating a state in the local search.\n",
        "        (The higher the better)\n",
        "        :param state: a state\n",
        "        :return: the value of a state\n",
        "        \"\"\"\n",
        "        return self.max_conflicts - self.conflicts(state)\n",
        "\n",
        "    @staticmethod\n",
        "    def random():\n",
        "        \"\"\"\n",
        "        Generate a random chess with 8 queens\n",
        "        :return: a tuple with 8 elements\n",
        "        \"\"\"\n",
        "        chess = [random.randrange(0, 8) for _ in range(8)]\n",
        "        return tuple(chess)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_chess(state):\n",
        "        print('\\t', end='')\n",
        "        for number in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
        "            print(f\"|  {number}  \", end='')\n",
        "        print('|', end='')\n",
        "        print('\\n\\t_________________________________________________')\n",
        "\n",
        "        for row, letter in zip(range(8), ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']):\n",
        "            print(letter + '\\t', end='')\n",
        "            print('|', end='')\n",
        "\n",
        "            for queen in state:\n",
        "                if queen == row:\n",
        "                    print('  Q  ', end='')\n",
        "                else:\n",
        "                    print('     ', end='')\n",
        "                print('|', end='')\n",
        "            print('\\n', end='')\n",
        "            print('\\t_________________________________________________')\n"
      ],
      "metadata": {
        "id": "MXbZj_WLZ5yQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hill Climbing"
      ],
      "metadata": {
        "id": "rVRsN9KJa-UM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first local search algorithm is Hill Climbing. Given the _value_ function to provide a score associated to a state, this strategy keeps track of one current state. On each iteration, it moves to the neighboring state having the highest value. The algorithm ends when no neighboring state have higher values than the current one.\n",
        "\n",
        "Therefore, the algorithm could be implemented in the HillClimbing Class receiving in input the problem. Then, in the _run_ function, we follow these steps:\n",
        "1. We initialize a node with the initial state;\n",
        "\n",
        "We loop the following steps:\n",
        "\n",
        "2. we expand the current state by finding the neighbors with the _successors_ function;\n",
        "3. we select the neighbor having the maximum value returned by the _value_ function;\n",
        "4. we check if the value of the current state is the greater than the one of the selected neighbor. If so, we have a solution. Otherwise, we expand the selected neighbor.\n",
        "\n",
        "The implementation of the HillClimbing Class is below."
      ],
      "metadata": {
        "id": "Mnm12EfMa_8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HillClimbing:\n",
        "\n",
        "    def __init__(self, problem):\n",
        "        self.problem = problem\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Hill Climbing'\n",
        "\n",
        "    def run(self):\n",
        "        # initial node with initial state\n",
        "        node = Node(state=self.problem.initial_state,\n",
        "                    parent=None,\n",
        "                    action=None,\n",
        "                    cost=0,\n",
        "                    depth=0)\n",
        "\n",
        "        while True:\n",
        "            # the local search looks only to the nearest states\n",
        "            new_states = self.problem.successors(node.state)\n",
        "\n",
        "            # no more neighbours to explore\n",
        "            if not new_states:\n",
        "                return 'Stop', node.state\n",
        "\n",
        "            # select the best neighbour given the objective function\n",
        "            best_neighbor, best_action = max(new_states, key=lambda x: self.problem.value(x[0]))\n",
        "\n",
        "            # check if there is not any improvement\n",
        "            if self.problem.value(node.state) >= self.problem.value(best_neighbor):\n",
        "                return 'Ok', node.state\n",
        "\n",
        "            # next neighbour to explore\n",
        "            node = node.expand(state=best_neighbor,\n",
        "                               action=best_action,\n",
        "                               cost=1)\n"
      ],
      "metadata": {
        "id": "kB-qZVRkbOYW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulated Annealing"
      ],
      "metadata": {
        "id": "uYVM94jJhLl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hill Climbing algorithm has a drawback. This algorithm can get stuck on local maximum and plateau of the value function. In these cases, the algorithm reaches a point at which no progress is being made. Then, a solution which is not the best solution is returned.\n",
        "\n",
        "To overcome the limitations of Hill Climbing, the Simulated Annealing algorithm introduces a random component within the algorithm, by allowing some bad moves but gradually decreasing their size and frequency in time.\n",
        "\n",
        "We follow these steps:\n",
        "1. We initialize a node with the initial state;\n",
        "\n",
        "We loop the following steps:\n",
        "\n",
        "2. we expand the current state by finding the neighbors with the _successors_ function;\n",
        "3. we select a random neighbor;\n",
        "4. we check if the value of the neighbor state is the greater than the one of the current state (i.e., if their difference is positive). If so, we surely expand the neighbor. Otherwise, we decide to expand the neighbor with if a certain probability value is less than the score exp(-delta/temperature).\n",
        "\n",
        "Here, delta is the value difference between the selected neighbor and the current state. Temp is the temperature factor. This temperature is updated during the loop as a function of the time (i.e., the number of iterations), so that its value decreases as the time increases. In the implementation below, there are three possible examples to update the temperature:\n",
        "1. the time is decreased linearly according to a lambda factor;\n",
        "2. the time is decreased exponentially by a factor exp(-lambda * time).\n",
        "\n",
        "The iterations continue untile the temperature value is greater than a fixed threshold, tipically 0. Optionally, we can set a number of maximum iterations to be reached until we loop.\n",
        "\n",
        "The implementation of the HillClimbing Class is below."
      ],
      "metadata": {
        "id": "OskeMQuZhNmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class SimulatedAnnealing:\n",
        "\n",
        "    def __init__(self, problem, min_temp=0, max_time=100, lam=0.001):\n",
        "        self.problem = problem\n",
        "        self.min_temp = min_temp\n",
        "        self.max_time = max_time\n",
        "        self.lam = lam\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Simulated Annealing'\n",
        "\n",
        "    def linear_schedule(self, temp):\n",
        "        return temp - self.lam\n",
        "\n",
        "    def exponential_schedule(self, temp, time):\n",
        "        return temp * math.exp(-self.lam * time)\n",
        "\n",
        "    def run(self, initial_temp=100):\n",
        "        # set time at the beginning of the search\n",
        "        time = 0\n",
        "        temp = initial_temp\n",
        "\n",
        "        # initial node with initial state\n",
        "        node = Node(state=self.problem.initial_state,\n",
        "                    parent=None,\n",
        "                    action=None,\n",
        "                    cost=0,\n",
        "                    depth=0)\n",
        "\n",
        "        while temp > self.min_temp and time < self.max_time:\n",
        "            # the local search looks only to the nearest states\n",
        "            new_states = self.problem.successors(node.state)\n",
        "\n",
        "            # no more neighbours to explore\n",
        "            if not new_states:\n",
        "                return 'Stop', node\n",
        "\n",
        "            # select the best neighbour given the objective function\n",
        "            selected_neighbour, selected_action = random.choice(new_states)\n",
        "\n",
        "            score_diff = self.problem.value(selected_neighbour) - self.problem.value(node.state)\n",
        "\n",
        "            if score_diff > 0 or random.uniform(0, 1) < math.exp(score_diff / temp):\n",
        "                # new state to explore\n",
        "                node = node.expand(state=selected_neighbour,\n",
        "                                   action=selected_action,\n",
        "                                   cost=1)\n",
        "            # update temperature\n",
        "            temp = self.exponential_schedule(temp, time)\n",
        "            # update time\n",
        "            time += 1\n",
        "\n",
        "        print(f'temp: {temp}, time: {time}')\n",
        "        return 'Ok', node.state\n"
      ],
      "metadata": {
        "id": "5o3S7Z71hbbN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Genetic Algorithm"
      ],
      "metadata": {
        "id": "comd_4eBwEIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Genetic Algorithm is a strategy for local search inspired by the metaphor of natural selection in biology.\n",
        "\n",
        "There is a population of individuals (states), in which the fittest (highest value) individuals produce offspring (successor states) that populate the next generation.\n",
        "\n",
        "In this kind of algorithm, a state must explicitly be described as a sequence of characters, like strings or tuples. We can observe that our problem fits this kind of requirement, as we modeled a state as a tuple of 8 numbers.\n",
        "\n",
        "The algorithm works as follows:\n",
        "1. we initially generate a random population. Practically, it means generating a certain number of random states.\n",
        "2. upon this random population, we continue by generating a certain number of subsequent generations (composed by the current population).\n",
        "\n",
        "The generations are created according to three steps. We perform the following steps a number of times equals to the population's size:\n",
        "1. **selection**: we select two individuals (i.e., states) from the population in a random weighted way. \"Random weighted way\" means selecting the individuals in a random way. However, the individuals with a higher value score are more likely to be selected. We perform these operations in the _select_ function. This function returns two individuals from the population;\n",
        "2. **crossover**: given the two selected individuals, we combine randomly them. Combining them randomly means that we merge their sequence of characters taking the first part of the sequence from the first individual, the second one from the second individual. The split index is generated randomly. We perform these operations in the _crossover_ function. This function returns an offspring (i.e., a state).\n",
        "3. **mutation**: given the offspring from the crossover operations, we modify a gene of the offspring if a random probability is greater than a certain treshold. A gene is a character of the sequence that represents the state. Of course, the randomly modified gene must have an accepted value from the gene pool. The gene pool is the set of characters that a gene can assume. For instance, in the 8-queens problem, the accepted values is in the range 0,7 given the 8x8 chess board. Both the gene to modify and the value that it is going to assume are generated randomly.\n",
        "\n",
        "Once a new generation is created, the population is updated. The operations described above are repeated for a fixed number of generations. Once we have finished generating generations, we return the state having the highest value function score, that will be the solution."
      ],
      "metadata": {
        "id": "nqFT5fgdwIB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Genetic:\n",
        "\n",
        "    def __init__(self, problem, population=1000, generations=100, p_mutation=0.1, gene_pool=None):\n",
        "        self.problem = problem\n",
        "        self.population = population\n",
        "        self.generations = generations\n",
        "        self.couples = int(self.population / 2)\n",
        "        self.p_mutation = p_mutation\n",
        "        self.gene_pool = gene_pool\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Genetic'\n",
        "\n",
        "    def select(self, population):\n",
        "        fitnesses = list(map(self.problem.value, population))\n",
        "        return random.choices(population=population, weights=fitnesses, k=2)\n",
        "\n",
        "    def crossover(self, couple):\n",
        "        parent_a, parent_b = couple\n",
        "        split = random.randrange(0, len(parent_a))\n",
        "        return tuple(list(parent_a[:split]) + list(parent_b[split:]))\n",
        "\n",
        "    def mutation(self, state):\n",
        "        if random.uniform(0, 1) > self.p_mutation or self.gene_pool is None:\n",
        "            return state\n",
        "        new_state = list(state)\n",
        "        new_state[random.randrange(len(state))] = random.choice(self.gene_pool)\n",
        "        return tuple(new_state)\n",
        "\n",
        "    def run(self):\n",
        "        population = [self.problem.random() for _ in range(self.population)]\n",
        "        for e in range(self.generations):\n",
        "            best = max(population, key=lambda x: self.problem.value(x))\n",
        "            print(f'Generation: {e} - max score: {self.problem.value(best)}')\n",
        "            new_generation = [\n",
        "                self.mutation(\n",
        "                    self.crossover(\n",
        "                        self.select(population)\n",
        "                    )\n",
        "                )\n",
        "                for _ in range(self.population)]\n",
        "            population = new_generation\n",
        "        return 'ok', max(population, key=lambda x: self.problem.value(x))\n"
      ],
      "metadata": {
        "id": "XluwoeyywLnG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "lmnVVWMj38z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the algorithms."
      ],
      "metadata": {
        "id": "TUk-Mucs390U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# formulate the problem\n",
        "problem = EightQueensProblem()\n",
        "\n",
        "# search algorithm\n",
        "search = HillClimbing(problem=problem)\n",
        "\n",
        "# run algorithm\n",
        "result, state = search.run()\n",
        "\n",
        "# display the solutions\n",
        "print(search)\n",
        "print(result)\n",
        "print(problem.value(state))\n",
        "print(state)\n",
        "problem.print_chess(state)\n",
        "\n",
        "# search algorithm\n",
        "search = SimulatedAnnealing(problem=problem, max_time=1000, lam=0.01)\n",
        "\n",
        "# run algorithm\n",
        "result, state = search.run()\n",
        "\n",
        "# display the solutions\n",
        "print(search)\n",
        "print(result)\n",
        "print(problem.value(state))\n",
        "print(state)\n",
        "problem.print_chess(state)\n",
        "\n",
        "# search algorithm\n",
        "search = Genetic(problem=problem, population=50, generations=100, p_mutation=0.1, gene_pool=list(range(8)))\n",
        "\n",
        "# run algorithm\n",
        "result, state = search.run()\n",
        "\n",
        "# display the solutions\n",
        "print(search)\n",
        "print(result)\n",
        "print(problem.value(state))\n",
        "print(state)\n",
        "problem.print_chess(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKQWD-fm4AeH",
        "outputId": "77798289-ffdd-49d3-ca34-cd4c876bb88a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hill Climbing\n",
            "Ok\n",
            "27\n",
            "(6, 0, 3, 1, 5, 7, 2, 4)\n",
            "\t|  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |\n",
            "\t_________________________________________________\n",
            "A\t|     |  Q  |     |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "B\t|     |     |     |  Q  |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "C\t|     |     |     |     |     |     |  Q  |     |\n",
            "\t_________________________________________________\n",
            "D\t|     |     |  Q  |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "E\t|     |     |     |     |     |     |     |  Q  |\n",
            "\t_________________________________________________\n",
            "F\t|     |     |     |     |  Q  |     |     |     |\n",
            "\t_________________________________________________\n",
            "G\t|  Q  |     |     |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "H\t|     |     |     |     |     |  Q  |     |     |\n",
            "\t_________________________________________________\n",
            "temp: 0.0, time: 388\n",
            "Simulated Annealing\n",
            "Ok\n",
            "28\n",
            "(4, 6, 0, 2, 7, 5, 3, 1)\n",
            "\t|  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |\n",
            "\t_________________________________________________\n",
            "A\t|     |     |  Q  |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "B\t|     |     |     |     |     |     |     |  Q  |\n",
            "\t_________________________________________________\n",
            "C\t|     |     |     |  Q  |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "D\t|     |     |     |     |     |     |  Q  |     |\n",
            "\t_________________________________________________\n",
            "E\t|  Q  |     |     |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "F\t|     |     |     |     |     |  Q  |     |     |\n",
            "\t_________________________________________________\n",
            "G\t|     |  Q  |     |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "H\t|     |     |     |     |  Q  |     |     |     |\n",
            "\t_________________________________________________\n",
            "Generation: 0 - max score: 25\n",
            "Generation: 1 - max score: 24\n",
            "Generation: 2 - max score: 24\n",
            "Generation: 3 - max score: 25\n",
            "Generation: 4 - max score: 25\n",
            "Generation: 5 - max score: 25\n",
            "Generation: 6 - max score: 24\n",
            "Generation: 7 - max score: 24\n",
            "Generation: 8 - max score: 26\n",
            "Generation: 9 - max score: 26\n",
            "Generation: 10 - max score: 26\n",
            "Generation: 11 - max score: 26\n",
            "Generation: 12 - max score: 25\n",
            "Generation: 13 - max score: 24\n",
            "Generation: 14 - max score: 25\n",
            "Generation: 15 - max score: 25\n",
            "Generation: 16 - max score: 26\n",
            "Generation: 17 - max score: 25\n",
            "Generation: 18 - max score: 25\n",
            "Generation: 19 - max score: 25\n",
            "Generation: 20 - max score: 25\n",
            "Generation: 21 - max score: 25\n",
            "Generation: 22 - max score: 25\n",
            "Generation: 23 - max score: 25\n",
            "Generation: 24 - max score: 25\n",
            "Generation: 25 - max score: 26\n",
            "Generation: 26 - max score: 25\n",
            "Generation: 27 - max score: 26\n",
            "Generation: 28 - max score: 26\n",
            "Generation: 29 - max score: 25\n",
            "Generation: 30 - max score: 25\n",
            "Generation: 31 - max score: 25\n",
            "Generation: 32 - max score: 25\n",
            "Generation: 33 - max score: 26\n",
            "Generation: 34 - max score: 25\n",
            "Generation: 35 - max score: 25\n",
            "Generation: 36 - max score: 25\n",
            "Generation: 37 - max score: 25\n",
            "Generation: 38 - max score: 26\n",
            "Generation: 39 - max score: 26\n",
            "Generation: 40 - max score: 26\n",
            "Generation: 41 - max score: 26\n",
            "Generation: 42 - max score: 26\n",
            "Generation: 43 - max score: 26\n",
            "Generation: 44 - max score: 26\n",
            "Generation: 45 - max score: 26\n",
            "Generation: 46 - max score: 25\n",
            "Generation: 47 - max score: 26\n",
            "Generation: 48 - max score: 25\n",
            "Generation: 49 - max score: 25\n",
            "Generation: 50 - max score: 26\n",
            "Generation: 51 - max score: 26\n",
            "Generation: 52 - max score: 26\n",
            "Generation: 53 - max score: 25\n",
            "Generation: 54 - max score: 26\n",
            "Generation: 55 - max score: 25\n",
            "Generation: 56 - max score: 25\n",
            "Generation: 57 - max score: 26\n",
            "Generation: 58 - max score: 26\n",
            "Generation: 59 - max score: 25\n",
            "Generation: 60 - max score: 26\n",
            "Generation: 61 - max score: 25\n",
            "Generation: 62 - max score: 26\n",
            "Generation: 63 - max score: 26\n",
            "Generation: 64 - max score: 26\n",
            "Generation: 65 - max score: 26\n",
            "Generation: 66 - max score: 26\n",
            "Generation: 67 - max score: 26\n",
            "Generation: 68 - max score: 26\n",
            "Generation: 69 - max score: 26\n",
            "Generation: 70 - max score: 27\n",
            "Generation: 71 - max score: 26\n",
            "Generation: 72 - max score: 26\n",
            "Generation: 73 - max score: 25\n",
            "Generation: 74 - max score: 26\n",
            "Generation: 75 - max score: 26\n",
            "Generation: 76 - max score: 26\n",
            "Generation: 77 - max score: 26\n",
            "Generation: 78 - max score: 25\n",
            "Generation: 79 - max score: 25\n",
            "Generation: 80 - max score: 25\n",
            "Generation: 81 - max score: 26\n",
            "Generation: 82 - max score: 26\n",
            "Generation: 83 - max score: 26\n",
            "Generation: 84 - max score: 26\n",
            "Generation: 85 - max score: 26\n",
            "Generation: 86 - max score: 26\n",
            "Generation: 87 - max score: 26\n",
            "Generation: 88 - max score: 26\n",
            "Generation: 89 - max score: 26\n",
            "Generation: 90 - max score: 26\n",
            "Generation: 91 - max score: 26\n",
            "Generation: 92 - max score: 26\n",
            "Generation: 93 - max score: 27\n",
            "Generation: 94 - max score: 26\n",
            "Generation: 95 - max score: 26\n",
            "Generation: 96 - max score: 26\n",
            "Generation: 97 - max score: 26\n",
            "Generation: 98 - max score: 26\n",
            "Generation: 99 - max score: 26\n",
            "Genetic\n",
            "ok\n",
            "26\n",
            "(1, 3, 7, 6, 3, 5, 2, 4)\n",
            "\t|  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |\n",
            "\t_________________________________________________\n",
            "A\t|     |     |     |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "B\t|  Q  |     |     |     |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "C\t|     |     |     |     |     |     |  Q  |     |\n",
            "\t_________________________________________________\n",
            "D\t|     |  Q  |     |     |  Q  |     |     |     |\n",
            "\t_________________________________________________\n",
            "E\t|     |     |     |     |     |     |     |  Q  |\n",
            "\t_________________________________________________\n",
            "F\t|     |     |     |     |     |  Q  |     |     |\n",
            "\t_________________________________________________\n",
            "G\t|     |     |     |  Q  |     |     |     |     |\n",
            "\t_________________________________________________\n",
            "H\t|     |     |  Q  |     |     |     |     |     |\n",
            "\t_________________________________________________\n"
          ]
        }
      ]
    }
  ]
}